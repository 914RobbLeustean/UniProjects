{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95459,"databundleVersionId":11347305,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, QuantileTransformer\nfrom sklearn.metrics import roc_auc_score\nimport catboost as cb\nimport xgboost as xgb\nimport lightgbm as lgb\nimport optuna\nfrom scipy import stats\nfrom itertools import combinations\n\nwarnings.filterwarnings('ignore')\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\"libraries loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:34:36.065076Z","iopub.execute_input":"2025-06-08T13:34:36.066040Z","iopub.status.idle":"2025-06-08T13:34:36.072652Z","shell.execute_reply.started":"2025-06-08T13:34:36.066009Z","shell.execute_reply":"2025-06-08T13:34:36.071571Z"}},"outputs":[{"name":"stdout","text":"libraries loaded\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Load data\ntrain = pd.read_csv('/kaggle/input/lateral-ai-academy-kaggle-competition/train.csv')\ntest = pd.read_csv('/kaggle/input/lateral-ai-academy-kaggle-competition/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/lateral-ai-academy-kaggle-competition/sample_submission.csv')\n\n# Critical alignment\ntrain['Tenure'] = np.clip(train['Tenure'], 0, 10)\n\nprint(f\"Data loaded: {train.shape[0]} train, {test.shape[0]} test\")\nprint(f\"Sample submission shape: {sample_submission.shape}\")\nprint(f\"Sample submission columns: {list(sample_submission.columns)}\")\n\n# Verify ID alignment\nassert len(sample_submission) == len(test), \"Sample submission and test size mismatch!\"\nprint(\"ID alignment verified\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:34:38.431022Z","iopub.execute_input":"2025-06-08T13:34:38.432021Z","iopub.status.idle":"2025-06-08T13:34:38.499678Z","shell.execute_reply.started":"2025-06-08T13:34:38.431984Z","shell.execute_reply":"2025-06-08T13:34:38.498753Z"}},"outputs":[{"name":"stdout","text":"Data loaded: 15000 train, 10000 test\nSample submission shape: (10000, 2)\nSample submission columns: ['id', 'Exited']\nID alignment verified\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def targeted_leak_fix(train_df, test_df):\n\n    def apply_targeted_fix(train_df, test_df):\n        train_result = train_df.copy()\n        test_result = test_df.copy()\n        \n        # Combine for global rankings\n        combined_df = pd.concat([train_df, test_df], ignore_index=True)\n        \n        print(\"Applying feature engineering with targeted fixes\")\n    \n        # Age features \n        combined_df['age_rank_global'] = combined_df['Age'].rank(pct=True)\n        combined_df['age_bins_5'] = pd.cut(combined_df['Age'], bins=5, labels=[0,1,2,3,4]).astype(int)\n        combined_df['age_bins_10'] = pd.cut(combined_df['Age'], bins=10, labels=range(10)).astype(int)\n        combined_df['age_squared'] = combined_df['Age'] ** 2\n        combined_df['age_cubed'] = combined_df['Age'] ** 3\n        combined_df['age_log'] = np.log1p(combined_df['Age'])\n        combined_df['age_sqrt'] = np.sqrt(combined_df['Age'])\n        combined_df['age_young'] = (combined_df['Age'] < 30).astype(int)\n        combined_df['age_adult'] = ((combined_df['Age'] >= 30) & (combined_df['Age'] < 40)).astype(int)\n        combined_df['age_middle'] = ((combined_df['Age'] >= 40) & (combined_df['Age'] < 50)).astype(int)\n        combined_df['age_senior'] = (combined_df['Age'] >= 50).astype(int)\n        combined_df['age_high_risk'] = (combined_df['Age'] >= 45).astype(int)\n        \n        # Balance features\n        combined_df['balance_rank_global'] = combined_df['Balance'].rank(pct=True)\n        combined_df['balance_log'] = np.log1p(combined_df['Balance'])\n        combined_df['balance_sqrt'] = np.sqrt(combined_df['Balance'])\n        combined_df['is_zero_balance'] = (combined_df['Balance'] == 0).astype(int)\n        combined_df['balance_high'] = (combined_df['Balance'] > 100000).astype(int)\n        combined_df['balance_medium'] = ((combined_df['Balance'] > 50000) & (combined_df['Balance'] <= 100000)).astype(int)\n        combined_df['balance_low'] = ((combined_df['Balance'] > 0) & (combined_df['Balance'] <= 50000)).astype(int)\n        \n        # Credit features - FIXED \n        combined_df['credit_rank_global'] = combined_df['CreditScore'].rank(pct=True)\n        \n        # Salary features - FIXED   \n        combined_df['salary_rank_global'] = combined_df['EstimatedSalary'].rank(pct=True)\n        \n        # Tenure features - FIXED \n        combined_df['tenure_rank_global'] = combined_df['Tenure'].rank(pct=True)\n        \n        # NumOfProducts patterns (safe)\n        combined_df['single_product'] = (combined_df['NumOfProducts'] == 1).astype(int)\n        combined_df['dual_product'] = (combined_df['NumOfProducts'] == 2).astype(int)\n        combined_df['multi_product'] = (combined_df['NumOfProducts'] >= 3).astype(int)\n        \n        # Interactions using GLOBAL RANKINGS\n        combined_df['age_balance_interaction'] = combined_df['age_rank_global'] * combined_df['balance_rank_global']\n        combined_df['age_products_interaction'] = combined_df['Age'] * combined_df['NumOfProducts']\n        combined_df['age_active_interaction'] = combined_df['Age'] * combined_df['IsActiveMember']\n        combined_df['age_credit_interaction'] = combined_df['Age'] * combined_df['CreditScore']\n        combined_df['balance_products_interaction'] = np.log1p(combined_df['Balance']) * combined_df['NumOfProducts']\n        combined_df['balance_active_interaction'] = np.log1p(combined_df['Balance']) * combined_df['IsActiveMember']\n        \n        # Ratios (safe)\n        combined_df['balance_per_product'] = combined_df['Balance'] / (combined_df['NumOfProducts'] + 1)\n        combined_df['balance_age_ratio'] = combined_df['Balance'] / (combined_df['Age'] + 1)\n        combined_df['credit_age_ratio'] = combined_df['CreditScore'] / (combined_df['Age'] + 1)\n        combined_df['salary_balance_ratio'] = (combined_df['EstimatedSalary'] + 1) / (combined_df['Balance'] + 1)\n        \n        # High-risk segments (safe)\n        combined_df['senior_single_product'] = ((combined_df['Age'] >= 45) & (combined_df['NumOfProducts'] == 1)).astype(int)\n        combined_df['high_balance_single'] = ((combined_df['Balance'] > 100000) & (combined_df['NumOfProducts'] == 1)).astype(int)\n        combined_df['senior_inactive'] = ((combined_df['Age'] >= 50) & (combined_df['IsActiveMember'] == 0)).astype(int)\n        combined_df['young_multi_product'] = ((combined_df['Age'] < 35) & (combined_df['NumOfProducts'] >= 2)).astype(int)\n        combined_df['middle_age_high_balance'] = ((combined_df['Age'] >= 40) & (combined_df['Age'] < 55) & (combined_df['Balance'] > 75000)).astype(int)\n        \n        # Tenure patterns using global ranking\n        combined_df['tenure_age_ratio'] = combined_df['tenure_rank_global'] / (combined_df['age_rank_global'] + 0.01)\n        combined_df['new_customer'] = (combined_df['Tenure'] <= 2).astype(int)\n        combined_df['loyal_customer'] = (combined_df['Tenure'] >= 7).astype(int)\n        \n        # Quantile transformations using global data\n        from sklearn.preprocessing import QuantileTransformer\n        qt = QuantileTransformer(output_distribution='normal', random_state=42)\n        combined_df['age_quantile_norm'] = qt.fit_transform(combined_df[['Age']]).flatten()\n        combined_df['balance_quantile_norm'] = qt.fit_transform(combined_df[['Balance']]).flatten()\n        combined_df['credit_quantile_norm'] = qt.fit_transform(combined_df[['CreditScore']]).flatten()\n        \n        # Polynomial combinations using global rankings\n        combined_df['age_balance_poly'] = combined_df['age_rank_global'] * combined_df['balance_rank_global']\n        combined_df['age_products_balance'] = combined_df['Age'] * combined_df['NumOfProducts'] * np.log1p(combined_df['Balance']) / 1000\n        \n        # Categorical encoding - CAREFUL ITS SO RETARDED\n        from sklearn.preprocessing import LabelEncoder\n        le_geo = LabelEncoder()\n        le_gender = LabelEncoder()\n        combined_df['Geography_encoded'] = le_geo.fit_transform(combined_df['Geography'])\n        combined_df['Gender_encoded'] = le_gender.fit_transform(combined_df['Gender'])\n        \n        # Target encoding - VERY heavy smoothing to prevent leakage\n        train_n = len(train_df)\n        train_target = train_df['Exited']\n        global_mean = train_target.mean()\n        \n        for col in ['Geography', 'Gender']:\n            category_stats = train_df.groupby(col)['Exited'].agg(['mean', 'count'])\n            \n            smoothing = 500  # heavier than before\n            \n            category_encoding = {}\n            for category in combined_df[col].unique():\n                if category in category_stats.index:\n                    cat_mean = category_stats.loc[category, 'mean']\n                    cat_count = category_stats.loc[category, 'count']\n                    smoothed_mean = (cat_mean * cat_count + global_mean * smoothing) / (cat_count + smoothing)\n                else:\n                    smoothed_mean = global_mean\n                \n                category_encoding[category] = smoothed_mean\n            \n            combined_df[f'{col}_target_enc'] = combined_df[col].map(category_encoding)\n        \n        # Split back\n        train_result = combined_df.iloc[:train_n].copy()\n        test_result = combined_df.iloc[train_n:].copy()\n        \n        return train_result, test_result\n    \n    train_fixed, test_fixed = apply_targeted_fix(train_df, test_df)\n    \n    # Remove the ID leak entirely\n    feature_cols = [col for col in train_fixed.columns \n                   if col not in ['Exited', 'CustomerId', 'Surname', 'Geography', 'Gender', 'id']]\n    \n    X_train_fixed = train_fixed[feature_cols].fillna(0)\n    X_test_fixed = test_fixed[feature_cols].fillna(0)\n    \n    # Clean data\n    for col in X_train_fixed.columns:\n        X_train_fixed[col] = pd.to_numeric(X_train_fixed[col], errors='coerce')\n        X_test_fixed[col] = pd.to_numeric(X_test_fixed[col], errors='coerce')\n    \n    X_train_fixed = X_train_fixed.fillna(0).replace([np.inf, -np.inf], 0)\n    X_test_fixed = X_test_fixed.fillna(0).replace([np.inf, -np.inf], 0)\n    \n    print(f\"Feature count after targeted fix: {len(feature_cols)}\")\n    \n    return train_fixed, test_fixed, X_train_fixed, X_test_fixed, feature_cols\n\n# Apply\ntrain_targeted, test_targeted, X_train_targeted, X_test_targeted, feature_names = targeted_leak_fix(train, test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:34:44.172268Z","iopub.execute_input":"2025-06-08T13:34:44.172557Z","iopub.status.idle":"2025-06-08T13:34:44.393596Z","shell.execute_reply.started":"2025-06-08T13:34:44.172538Z","shell.execute_reply":"2025-06-08T13:34:44.392610Z"}},"outputs":[{"name":"stdout","text":"Applying feature engineering with targeted fixes\nFeature count after targeted fix: 60\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def test_predictive_power_with_current_fix(X_train, X_test, y_train):\n    \"\"\"\n    Test if 0.8092 adversarial AUC is acceptable for maintaining predictive power.\n    Sometimes slight distribution differences don't hurt leaderboard performance.\n    \"\"\"\n    \n    print(\"=== TESTING PREDICTIVE POWER WITH 0.8092 ADVERSARIAL AUC ===\")\n    \n    # Use your original best parameters\n    best_params = {\n        'iterations': 1270,\n        'depth': 4,\n        'learning_rate': 0.010092848351022131,\n        'l2_leaf_reg': 12.908182021977488,\n        'border_count': 128,\n        'bagging_temperature': 0.4859047927134544,\n        'random_strength': 0.6121265192307465,\n        'random_state': 42,\n        'verbose': False\n    }\n    \n    # 5-fold cross-validation\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = []\n    oof_predictions = np.zeros(len(X_train))\n    \n    print(\"Running 5-fold cross-validation with leak-fixed features...\")\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n        fold_model = cb.CatBoostClassifier(**best_params)\n        \n        fold_model.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n        val_pred = fold_model.predict_proba(X_train.iloc[val_idx])[:, 1]\n        oof_predictions[val_idx] = val_pred\n        \n        score = roc_auc_score(y_train.iloc[val_idx], val_pred)\n        cv_scores.append(score)\n        print(f\"Fold {fold + 1}: {score:.4f}\")\n    \n    mean_cv = np.mean(cv_scores)\n    std_cv = np.std(cv_scores)\n    overall_cv = roc_auc_score(y_train, oof_predictions)\n    \n    print(f\"\\nRESULTS WITH LEAK-FIXED FEATURES:\")\n    print(f\"Fold CV: {mean_cv:.4f} ± {std_cv:.4f}\")\n    print(f\"Overall CV: {overall_cv:.4f}\")\n    print(f\"Original CV: 0.9345\")\n    print(f\"Performance change: {overall_cv - 0.9345:.4f}\")\n    print(f\"Adversarial AUC: 0.8092 (improved from 1.0000)\")\n    \n    # Decision logic\n    performance_loss = 0.9345 - overall_cv\n    \n    if performance_loss < 0.008:  # Less than 0.008 loss is acceptable\n        print(f\"\\nDECISION: ACCEPTABLE PERFORMANCE LOSS ({performance_loss:.4f})\")\n        print(\"Proceeding with leak-fixed features...\")\n        \n        # Train final model\n        final_model = cb.CatBoostClassifier(**best_params)\n        print(\"Training final model on all data...\")\n        final_model.fit(X_train, y_train)\n        \n        # Generate predictions\n        final_predictions = final_model.predict_proba(X_test)[:, 1]\n        final_predictions = np.clip(final_predictions, 0.001, 0.999)\n        \n        # Create submission\n        submission = sample_submission.copy()\n        submission['Exited'] = final_predictions\n        submission.to_csv('leak_fixed_submission.csv', index=False)\n        \n        print(f\"\\nSUBMISSION CREATED:\")\n        print(f\"  CV Score: {overall_cv:.4f}\")\n        print(f\"  Adversarial AUC: 0.8092\")\n        print(f\"  Expected LB range: {overall_cv - 0.004:.4f} to {overall_cv + 0.002:.4f}\")\n        print(f\"  Target to beat: 0.93561\")\n        \n        # Check if likely to beat target\n        conservative_lb = overall_cv - 0.004\n        optimistic_lb = overall_cv + 0.002\n        \n        if conservative_lb > 0.93561:\n            print(\"  VERY LIKELY TO BEAT TARGET!\")\n        elif optimistic_lb > 0.93561:\n            print(\"  LIKELY TO BEAT TARGET!\")\n        else:\n            print(\"  MAY NOT BEAT TARGET - consider more optimization\")\n        \n        print(f\"\\nPrediction statistics:\")\n        print(f\"  Range: [{final_predictions.min():.4f}, {final_predictions.max():.4f}]\")\n        print(f\"  Mean: {final_predictions.mean():.4f}\")\n        print(\"First 5 predictions:\")\n        print(submission.head())\n        \n        return True, overall_cv, final_predictions\n        \n    else:\n        print(f\"\\nDECISION: PERFORMANCE LOSS TOO HIGH ({performance_loss:.4f})\")\n        print(\"Need more aggressive optimization or alternative approaches\")\n        return False, overall_cv, None\n\n# Test with current fix\nsuccess, cv_score, predictions = test_predictive_power_with_current_fix(\n    X_train_targeted, X_test_targeted, train['Exited']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:36:56.484273Z","iopub.execute_input":"2025-06-08T13:36:56.484659Z","iopub.status.idle":"2025-06-08T13:37:32.992317Z","shell.execute_reply.started":"2025-06-08T13:36:56.484633Z","shell.execute_reply":"2025-06-08T13:37:32.990995Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"=== TESTING PREDICTIVE POWER WITH 0.8092 ADVERSARIAL AUC ===\nRunning 5-fold cross-validation with leak-fixed features...\nFold 1: 0.9257\nFold 2: 0.9271\nFold 3: 0.9486\nFold 4: 0.9350\nFold 5: 0.9369\n\nRESULTS WITH LEAK-FIXED FEATURES:\nFold CV: 0.9347 ± 0.0082\nOverall CV: 0.9346\nOriginal CV: 0.9345\nPerformance change: 0.0001\nAdversarial AUC: 0.8092 (improved from 1.0000)\n\nDECISION: ACCEPTABLE PERFORMANCE LOSS (-0.0001)\nProceeding with leak-fixed features...\nTraining final model on all data...\n\nSUBMISSION CREATED:\n  CV Score: 0.9346\n  Adversarial AUC: 0.8092\n  Expected LB range: 0.9306 to 0.9366\n  Target to beat: 0.93561\n  LIKELY TO BEAT TARGET!\n\nPrediction statistics:\n  Range: [0.0022, 0.9977]\n  Mean: 0.2023\nFirst 5 predictions:\n      id    Exited\n0  15000  0.887699\n1  15001  0.052068\n2  15002  0.021671\n3  15003  0.015158\n4  15004  0.100993\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import os\n\nfile_path = '/kaggle/working/lyabababad_submission.csv'\n\n# Check if file exists before removing\nif os.path.exists(file_path):\n    os.remove(file_path)\n    print(f\"{file_path} has been deleted.\")\nelse:\n    print(f\"{file_path} does not exist.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T13:40:53.354864Z","iopub.execute_input":"2025-06-08T13:40:53.355223Z","iopub.status.idle":"2025-06-08T13:40:53.361471Z","shell.execute_reply.started":"2025-06-08T13:40:53.355182Z","shell.execute_reply":"2025-06-08T13:40:53.360291Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/lyabababad_submission.csv has been deleted.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}